# ===================================================================
# LLM PROVIDER SELECTION
# ===================================================================
# Choose your primary LLM provider. Options:
#   auto          - Infer from available API keys (default)
#   anthropic     - Claude models via Anthropic API
#   ollama-local  - Self-hosted Ollama (http://localhost:11434)
#   ollama-cloud  - Hosted Ollama with API key
#   openai        - OpenAI GPT models
#   groq          - Groq fast inference
#
# MODEL_PROVIDER=auto

# Model name override (optional — uses provider default if not set)
# Examples: claude-sonnet-4-5, llama3.1:8b, gpt-4o, llama-3.3-70b-versatile
# MODEL_NAME=

# Fallback chain (optional — comma-separated provider names)
# If primary fails, tries each fallback in order. Empty = fail-fast.
# MODEL_FALLBACK_CHAIN=ollama-local,openai

# Per-agent model overrides (optional — JSON dict)
# Keys are agent names, values are model strings.
# Use "provider:model" for cross-provider (e.g., "anthropic:claude-sonnet-4-5").
# Plain model names use the global provider (e.g., "llama3.1:8b").
# Known agents: recall, ask, learn, create, review, chief_of_staff,
#   coach, pmo, email, specialist, clarity, synthesizer, template_builder
# AGENT_MODEL_OVERRIDES={"recall": "llama3.1:8b", "create": "anthropic:claude-sonnet-4-5"}

# ===================================================================
# PROVIDER CREDENTIALS
# ===================================================================

# --- Anthropic (Claude) ---
ANTHROPIC_API_KEY=your-anthropic-api-key

# --- OpenAI ---
# Used for Mem0 embeddings AND as LLM provider when MODEL_PROVIDER=openai
OPENAI_API_KEY=your-openai-api-key
# OPENAI_MODEL_NAME=gpt-4o

# --- Ollama ---
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
# OLLAMA_API_KEY=         # Required for ollama-cloud only

# --- Groq ---
# GROQ_API_KEY=your-groq-api-key
# GROQ_MODEL_NAME=llama-3.3-70b-versatile

# ===================================================================
# SUBSCRIPTION AUTH (Claude Pro/Max)
# ===================================================================
# Use your Claude Pro/Max subscription instead of API credits.
# Routes LLM calls through claude CLI using OAuth token auth.
#
# Prerequisites:
#   1. Install claude CLI: npm install -g @anthropic-ai/claude-code
#   2. Authenticate: run `claude` and complete the login flow
#   3. Set USE_SUBSCRIPTION=true below
#
# USE_SUBSCRIPTION=true
# CLAUDE_OAUTH_TOKEN=sk-ant-oat01-your-token-here

# ===================================================================
# MEMORY & STORAGE
# ===================================================================

# Mem0 Cloud API key (falls back to local Qdrant if not set)
MEM0_API_KEY=

# Mem0 Cloud advanced retrieval options
# MEM0_KEYWORD_SEARCH=true           # Hybrid keyword+semantic search (+10ms latency)
# MEM0_RERANK=true                   # Native reranking for better ordering (+150-200ms latency)
# MEM0_FILTER_MEMORIES=false         # LLM-based memory filtering for high precision (+200-300ms latency)
# MEM0_USE_CRITERIA=true              # Enable Criteria Retrieval scoring (requires setup_criteria_retrieval() first). +300ms+ latency.

# Required: Supabase project credentials
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key

# ===================================================================
# GRAPH MEMORY (optional)
# ===================================================================
# Provider: none (default), mem0 (requires Mem0 Pro), graphiti (requires Neo4j)
# GRAPH_PROVIDER=none
# NEO4J_URL=neo4j+s://xxx.databases.neo4j.io
# NEO4J_USERNAME=neo4j
# NEO4J_PASSWORD=your-password

# Graphiti Knowledge Graph
# GRAPHITI_ENABLED=false
# GRAPHITI_EMBEDDING_MODEL=voyage-3.5
# GRAPHITI_LLM_MODEL=deepseek-v3.1:671b-cloud
# FALKORDB_URL=
# FALKORDB_PASSWORD=

# ===================================================================
# EMBEDDINGS & SEARCH
# ===================================================================

# Voyage AI (Primary Embeddings + Reranking)
# Get key at: https://dash.voyageai.com/
# VOYAGE_API_KEY=pa-your-voyage-key

# ===================================================================
# BRAIN CONFIG
# ===================================================================
BRAIN_USER_ID=your-user-id
BRAIN_DATA_PATH=/path/to/your/brain-data

# Multi-user voice isolation (comma-separated user IDs)
# Each user gets their own voice profile, writing style, and content examples.
# Shared data (memories, patterns, knowledge) remains accessible to all users.
# ALLOWED_USER_IDS=uttam,robert,luke,brainforge

# REST API authentication (optional -- leave empty for local-only use)
# BRAIN_API_KEY=your-secret-api-key

# ===================================================================
# VAULT INGESTION (optional)
# ===================================================================
# VAULT_PATH=C:\path\to\your\vault
# VAULT_INGESTION_BATCH_SIZE=20
# VAULT_INGESTION_CONCURRENCY=5

# ===================================================================
# HNSW VECTOR SEARCH TUNING
# ===================================================================
# HNSW_EF_SEARCH=100                    # Query-time recall vs latency (10-500, default: 100)

# ===================================================================
# SERVICE TIMEOUTS
# ===================================================================
# SERVICE_TIMEOUT_SECONDS=15             # Per-call timeout for Mem0/Supabase (1-60, default: 15)
# Mem0 cloud calls retry up to 3 times with exponential backoff on transient errors.
# Idle timeout detection: client re-instantiates after 4 min idle (Mem0 bug workaround).

# ===================================================================
# BATCH OPERATIONS
# ===================================================================
# BATCH_UPSERT_CHUNK_SIZE=500            # Max rows per Supabase batch upsert (1-1000, default: 500)

# ===================================================================
# MCP TRANSPORT (Docker)
# ===================================================================
# Transport mode:
#   stdio            - Local process (default). Claude Code spawns the server.
#   http             - Streamable HTTP for Docker/network. Serves at /mcp endpoint.
#   streamable-http  - Alias for http (same behavior in FastMCP 2.x).
#   sse              - Legacy SSE transport (deprecated by MCP spec 2025-03-26).
#
# Docker usage: set MCP_TRANSPORT=http, connect Claude Code to http://localhost:8000/mcp
# MCP_TRANSPORT=http
# MCP_HOST=0.0.0.0
# MCP_PORT=8000
